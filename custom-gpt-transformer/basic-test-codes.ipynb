{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac952fd",
   "metadata": {},
   "source": [
    "#### Torch Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e26f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693cc813",
   "metadata": {},
   "source": [
    "#### CUDA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3c84dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27eed6",
   "metadata": {},
   "source": [
    "#### Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a8a7dcf-c82f-4f76-8625-01974fb32265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the contents of the file 'infinite_in_modern_thought.txt' and store it in the variable 'text'\n",
    "with open('infinite_in_modern_thought.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print the first 1000 characters of the text\n",
    "# print(text[:1000])\n",
    "\n",
    "# Create a list of unique characters in the text and sort them\n",
    "chars = sorted(list(set(text)))\n",
    "# print(chars)\n",
    "\n",
    "# Print the number of unique characters\n",
    "# print(len(chars))\n",
    "\n",
    "# print(text[:1000])\n",
    "chars = sorted(list(set(text)))\n",
    "# print(chars)\n",
    "# print(len(chars))\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922e2a5",
   "metadata": {},
   "source": [
    "#### Simple Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "569fea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 61, 68, 68, 71, 11, 1, 50, 71, 74, 68, 60, 2]\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary that maps each character to its index in the 'chars' list\n",
    "string_to_index = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a dictionary that maps each index to its corresponding character in the 'chars' list\n",
    "index_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Define the 'endcode' function that converts a string to a list of character indices\n",
    "endcode = lambda s: [string_to_index[c] for c in s]\n",
    "\n",
    "# Define the 'decode' function that converts a list of character indices to a string\n",
    "decode = lambda l: ''.join([index_to_string[i] for i in l])\n",
    "\n",
    "print(endcode(\"Hello, World!\"))\n",
    "\n",
    "print(decode(endcode(\"Hello, World!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674aee3",
   "metadata": {},
   "source": [
    "#### Torch Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99153e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65, 76, 61,  1, 65, 70,  1, 69, 71, 60, 61, 74, 70,  1,\n",
      "        76, 64, 71, 77, 63, 64, 76,  0,  1,  1,  1,  1,  0, 47, 64, 65, 75,  1,\n",
      "        61, 58, 71, 71, 67,  1, 65, 75,  1, 62])\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary that maps each character to its index in the 'chars' list\n",
    "string_to_index = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a dictionary that maps each index to its corresponding character in the 'chars' list\n",
    "index_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Define the 'endcode' function that converts a string to a list of character indices\n",
    "endcode = lambda s: [string_to_index[c] for c in s]\n",
    "\n",
    "# Define the 'decode' function that converts a list of character indices to a string\n",
    "decode = lambda l: ''.join([index_to_string[i] for i in l])\n",
    "\n",
    "# Create a tensor from the encoded text using torch.tensor\n",
    "data = torch.tensor(endcode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ebb88",
   "metadata": {},
   "source": [
    "#### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33186e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets\n",
    "data_size = int(0.9 * len(data))  # Calculate the size of the train data\n",
    "\n",
    "train_data = data[:data_size]  # Assign the first 90% of the data to the train_data variable\n",
    "val_data = data[data_size:]  # Assign the remaining 10% of the data to the val_data variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aca5f",
   "metadata": {},
   "source": [
    "#### Tensor Process Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97864467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([97]), the target: 47\n",
      "when input is tensor([97, 47]), the target: 64\n",
      "when input is tensor([97, 47, 64]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1]), the target: 43\n",
      "when input is tensor([97, 47, 64, 61,  1, 43]), the target: 74\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74]), the target: 71\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71]), the target: 66\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    # Get the context by slicing the input sequence up to the current position\n",
    "    context = x[:t + 1]\n",
    "    \n",
    "    # Get the target by selecting the next character in the input sequence\n",
    "    target = y[t]\n",
    "    \n",
    "    # Print the context and target\n",
    "    print(f\"when input is {context}, the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248bc77",
   "metadata": {},
   "source": [
    "#### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc93cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "tensor([[64, 61,  1, 79, 64, 71, 68, 61],\n",
      "        [ 1,  7, 79, 64, 65, 59, 64,  1],\n",
      "        [ 0, 61, 80, 65, 75, 76, 61, 70],\n",
      "        [79, 57, 81,  1, 57, 75,  1, 76]], device='cuda:0')\n",
      "tensor([[61,  1, 79, 64, 71, 68, 61,  1],\n",
      "        [ 7, 79, 64, 65, 59, 64,  1, 79],\n",
      "        [61, 80, 65, 75, 76, 61, 70, 59],\n",
      "        [57, 81,  1, 57, 75,  1, 76, 64]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and validation sets\n",
    "data_size = int(0.9 * len(data))  # Calculate the size of the train data\n",
    "\n",
    "train_data = data[:data_size]  # Assign the first 90% of the data to the train_data variable\n",
    "val_data = data[data_size:]  # Assign the remaining 10% of the data to the val_data variable\n",
    "\n",
    "# Define the get_batch function that returns a batch of data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Get a batch of training data\n",
    "x, y = get_batch('train')\n",
    "\n",
    "# Print the shape of x and y\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# Print the values of x and y\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee42a2",
   "metadata": {},
   "source": [
    "#### Bigram (Autoregressive Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f77265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_interval)  # Initialize a tensor to store the losses\n",
    "        for i in range(eval_interval):\n",
    "            x, y = get_batch(split)  # Get a batch of data\n",
    "            logits, loss = model(x, y)  # Forward pass through the model\n",
    "            losses[i] = loss.item()  # Store the loss value\n",
    "        out[split] = losses.mean()  # Calculate the mean loss for the split\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc3ebc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R*cZf•lD(fa—YDRODbrêpé$0:w!n+U﻿_0QZ“ W;kf2N—ew•%bz*J…wF[?eaöNl!BQi+éôêO!H™&U™W﻿Dv﻿k[1T-WE2pAl”##na X\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    # Initialize the model with the vocabulary size\n",
    "    def __init__(self, vocab_size):\n",
    "        # Call the parent class's constructor\n",
    "        super().__init__()\n",
    "        # Create an embedding table to map token indices to vectors\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Get the logits (unnormalized probabilities) for the input indices\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        # If targets are not provided, set the loss to None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        # If targets are provided, calculate the cross-entropy loss\n",
    "        else:\n",
    "            # Get the shape of the logits tensor (batch size, sequence length, vocab size)\n",
    "            B, T, C = logits.shape\n",
    "            # Reshape the logits tensor to (batch size * sequence length, vocab size)\n",
    "            logits = logits.view(B*T, C)\n",
    "            # Reshape the targets tensor to (batch size * sequence length)\n",
    "            targets = targets.view(B*T)\n",
    "            # Calculate the cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        # Return the logits and loss\n",
    "        return logits, loss\n",
    "\n",
    "    # Define the generate method\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Generate new tokens one at a time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the logits and loss for the current input indices\n",
    "            logits, loss = self.forward(idx)\n",
    "            # Get the logits for the last token in the sequence\n",
    "            logits = logits[:, -1, :]\n",
    "            # Convert the logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample the next token from the probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append the next token to the current sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # Return the generated sequence\n",
    "        return idx\n",
    "\n",
    "# Create an instance of the BigramLanguageModel with the specified vocabulary size\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "# Move the model to the specified device (e.g. GPU)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a context tensor with a single token (index 0)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# Generate a sequence of 100 tokens starting from the context\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=100)[0].tolist())\n",
    "# Print the generated sequence\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cad24d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.0148, val loss 4.1414\n",
      "step 100: train loss 4.0100, val loss 4.0979\n",
      "step 200: train loss 4.0318, val loss 4.0931\n",
      "step 300: train loss 3.9532, val loss 4.0376\n",
      "step 400: train loss 3.9455, val loss 4.0262\n",
      "step 500: train loss 3.9304, val loss 4.0580\n",
      "step 600: train loss 3.8994, val loss 4.0182\n",
      "step 700: train loss 3.8809, val loss 4.0294\n",
      "step 800: train loss 3.8775, val loss 3.9768\n",
      "step 900: train loss 3.8537, val loss 3.9811\n",
      "step 1000: train loss 3.8524, val loss 3.9463\n",
      "step 1100: train loss 3.8164, val loss 3.9090\n",
      "step 1200: train loss 3.7919, val loss 3.9211\n",
      "step 1300: train loss 3.7717, val loss 3.9284\n",
      "step 1400: train loss 3.7755, val loss 3.9130\n",
      "step 1500: train loss 3.7795, val loss 3.8395\n",
      "step 1600: train loss 3.7286, val loss 3.8843\n",
      "step 1700: train loss 3.7139, val loss 3.8227\n",
      "step 1800: train loss 3.6971, val loss 3.8138\n",
      "step 1900: train loss 3.6886, val loss 3.8349\n",
      "step 2000: train loss 3.7093, val loss 3.8718\n",
      "step 2100: train loss 3.6681, val loss 3.7504\n",
      "step 2200: train loss 3.6615, val loss 3.7759\n",
      "step 2300: train loss 3.6014, val loss 3.7825\n",
      "step 2400: train loss 3.6218, val loss 3.7673\n",
      "step 2500: train loss 3.5777, val loss 3.7349\n",
      "step 2600: train loss 3.5726, val loss 3.6958\n",
      "step 2700: train loss 3.5396, val loss 3.6903\n",
      "step 2800: train loss 3.5438, val loss 3.6942\n",
      "step 2900: train loss 3.5201, val loss 3.7083\n",
      "step 3000: train loss 3.5049, val loss 3.6780\n",
      "step 3100: train loss 3.4822, val loss 3.6746\n",
      "step 3200: train loss 3.5189, val loss 3.6168\n",
      "step 3300: train loss 3.4852, val loss 3.6347\n",
      "step 3400: train loss 3.4944, val loss 3.6155\n",
      "step 3500: train loss 3.4766, val loss 3.6476\n",
      "step 3600: train loss 3.4323, val loss 3.6451\n",
      "step 3700: train loss 3.4317, val loss 3.5596\n",
      "step 3800: train loss 3.4096, val loss 3.5436\n",
      "step 3900: train loss 3.4243, val loss 3.5630\n",
      "step 4000: train loss 3.4142, val loss 3.5948\n",
      "step 4100: train loss 3.3734, val loss 3.5878\n",
      "step 4200: train loss 3.4000, val loss 3.5411\n",
      "step 4300: train loss 3.3308, val loss 3.5644\n",
      "step 4400: train loss 3.3512, val loss 3.5120\n",
      "step 4500: train loss 3.3267, val loss 3.5879\n",
      "step 4600: train loss 3.3142, val loss 3.5410\n",
      "step 4700: train loss 3.2888, val loss 3.4836\n",
      "step 4800: train loss 3.3292, val loss 3.4727\n",
      "step 4900: train loss 3.2900, val loss 3.4836\n",
      "3.5866475105285645\n"
     ]
    }
   ],
   "source": [
    "max_iter = 5000\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 100\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    # Get a batch of training data\n",
    "    x, y = get_batch('train')\n",
    "    # Get the logits and loss\n",
    "    logits, loss = model(x, y)\n",
    "    # Backpropagate the loss\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print the loss every 100 iterations\n",
    "    # if iter % 100 == 0:\n",
    "    #     print(f\"loss: {loss.item()}\")\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b42836",
   "metadata": {},
   "source": [
    "#### Some Optimization Algorithms and Loss Functions\n",
    "\n",
    "**MSE (Mean Squared Error):** MSE is a commonly used loss function in regression problems. It measures the average squared difference between the predicted and actual values. The lower the MSE, the better the model's performance.\n",
    "\n",
    "**GD (Gradient Descent):** GD is an optimization algorithm used to minimize the loss function of a model. It iteratively updates the model's parameters in the direction of steepest descent of the loss function. GD can be slow for large datasets as it requires computing the gradients for the entire dataset in each iteration.\n",
    "\n",
    "**Momentum:** Momentum is an extension of GD that helps accelerate convergence and overcome local minima. It introduces a momentum term that accumulates the gradients of previous iterations and uses it to update the model's parameters. This helps the model to continue moving in the right direction even when the gradients are small.\n",
    "\n",
    "**RMSprop (Root Mean Square Propagation):** RMSprop is an optimization algorithm that adapts the learning rate for each parameter based on the average of recent squared gradients. It helps to speed up convergence by reducing the learning rate for parameters with large gradients and increasing it for parameters with small gradients.\n",
    "\n",
    "**Adam:** Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of both momentum and RMSprop. It maintains a running average of both the gradients and the squared gradients, and uses them to update the model's parameters. Adam is known for its fast convergence and good performance on a wide range of problems.\n",
    "\n",
    "**AdamW:** AdamW is a variant of the Adam optimizer that incorporates weight decay regularization. Weight decay helps prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. AdamW is particularly effective when dealing with models with large numbers of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40251cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Y#Yâö4™_F[™SêmöV!1ây&K1Tm)K#DuHyag™o/l,x’-sisthqr[fDq.#+ks.#G“c+!YMO_O&$•—DéV,*J0Mmmes9wa7quF.dra wh\n"
     ]
    }
   ],
   "source": [
    "# Create a context tensor with a single token (index 0)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# Generate a sequence of 100 tokens starting from the context\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=100)[0].tolist())\n",
    "# Print the generated sequence\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc13008",
   "metadata": {},
   "source": [
    "#### Block Size and Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72bfb9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNOTE:\\n- The block size determines the size of each block in the tensor.\\n- The batch size determines the number of blocks to be processed in parallel.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "- The block size determines the size of each block in the tensor.\n",
    "- The batch size determines the number of blocks to be processed in parallel.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
