{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac952fd",
   "metadata": {},
   "source": [
    "#### Torch Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e26f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693cc813",
   "metadata": {},
   "source": [
    "#### CUDA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3c84dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27eed6",
   "metadata": {},
   "source": [
    "#### Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a8a7dcf-c82f-4f76-8625-01974fb32265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the contents of the file 'infinite_in_modern_thought.txt' and store it in the variable 'text'\n",
    "with open('infinite_in_modern_thought.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print the first 1000 characters of the text\n",
    "# print(text[:1000])\n",
    "\n",
    "# Create a list of unique characters in the text and sort them\n",
    "chars = sorted(list(set(text)))\n",
    "# print(chars)\n",
    "\n",
    "# Print the number of unique characters\n",
    "# print(len(chars))\n",
    "\n",
    "# print(text[:1000])\n",
    "chars = sorted(list(set(text)))\n",
    "# print(chars)\n",
    "# print(len(chars))\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922e2a5",
   "metadata": {},
   "source": [
    "#### Simple Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "569fea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 61, 68, 68, 71, 11, 1, 50, 71, 74, 68, 60, 2]\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary that maps each character to its index in the 'chars' list\n",
    "string_to_index = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a dictionary that maps each index to its corresponding character in the 'chars' list\n",
    "index_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Define the 'endcode' function that converts a string to a list of character indices\n",
    "endcode = lambda s: [string_to_index[c] for c in s]\n",
    "\n",
    "# Define the 'decode' function that converts a list of character indices to a string\n",
    "decode = lambda l: ''.join([index_to_string[i] for i in l])\n",
    "\n",
    "print(endcode(\"Hello, World!\"))\n",
    "\n",
    "print(decode(endcode(\"Hello, World!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674aee3",
   "metadata": {},
   "source": [
    "#### Torch Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99153e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65, 76, 61,  1, 65, 70,  1, 69, 71, 60, 61, 74, 70,  1,\n",
      "        76, 64, 71, 77, 63, 64, 76,  0,  1,  1,  1,  1,  0, 47, 64, 65, 75,  1,\n",
      "        61, 58, 71, 71, 67,  1, 65, 75,  1, 62])\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary that maps each character to its index in the 'chars' list\n",
    "string_to_index = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a dictionary that maps each index to its corresponding character in the 'chars' list\n",
    "index_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Define the 'endcode' function that converts a string to a list of character indices\n",
    "endcode = lambda s: [string_to_index[c] for c in s]\n",
    "\n",
    "# Define the 'decode' function that converts a list of character indices to a string\n",
    "decode = lambda l: ''.join([index_to_string[i] for i in l])\n",
    "\n",
    "# Create a tensor from the encoded text using torch.tensor\n",
    "data = torch.tensor(endcode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ebb88",
   "metadata": {},
   "source": [
    "#### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33186e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets\n",
    "data_size = int(0.9 * len(data))  # Calculate the size of the train data\n",
    "\n",
    "train_data = data[:data_size]  # Assign the first 90% of the data to the train_data variable\n",
    "val_data = data[data_size:]  # Assign the remaining 10% of the data to the val_data variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aca5f",
   "metadata": {},
   "source": [
    "#### Tensor Process Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97864467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([97]), the target: 47\n",
      "when input is tensor([97, 47]), the target: 64\n",
      "when input is tensor([97, 47, 64]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1]), the target: 43\n",
      "when input is tensor([97, 47, 64, 61,  1, 43]), the target: 74\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74]), the target: 71\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71]), the target: 66\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    # Get the context by slicing the input sequence up to the current position\n",
    "    context = x[:t + 1]\n",
    "    \n",
    "    # Get the target by selecting the next character in the input sequence\n",
    "    target = y[t]\n",
    "    \n",
    "    # Print the context and target\n",
    "    print(f\"when input is {context}, the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248bc77",
   "metadata": {},
   "source": [
    "#### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc93cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "tensor([[24, 13,  0,  0,  1,  1,  1,  1],\n",
      "        [ 1, 69, 61, 76, 64, 71, 60, 75],\n",
      "        [70, 60,  1, 71, 62,  1, 69, 71],\n",
      "        [63,  1, 71, 74,  1, 72, 74, 71]], device='cuda:0')\n",
      "tensor([[13,  0,  0,  1,  1,  1,  1,  1],\n",
      "        [69, 61, 76, 64, 71, 60, 75,  1],\n",
      "        [60,  1, 71, 62,  1, 69, 71, 76],\n",
      "        [ 1, 71, 74,  1, 72, 74, 71, 78]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and validation sets\n",
    "data_size = int(0.9 * len(data))  # Calculate the size of the train data\n",
    "\n",
    "train_data = data[:data_size]  # Assign the first 90% of the data to the train_data variable\n",
    "val_data = data[data_size:]  # Assign the remaining 10% of the data to the val_data variable\n",
    "\n",
    "# Define the get_batch function that returns a batch of data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Get a batch of training data\n",
    "x, y = get_batch('train')\n",
    "\n",
    "# Print the shape of x and y\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# Print the values of x and y\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee42a2",
   "metadata": {},
   "source": [
    "#### Bigram (Autoregressive Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc3ebc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UbuE.gQLSK(6,d’a%F9pNoes2[H +X.‘kOP([Ny:âcalZéqtt;7RwQ]Y?]f%J5ôl‘ARlY%KM6LVé]qf‘6:96csm27 :wT)Oz3wmæ\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    # Initialize the model with the vocabulary size\n",
    "    def __init__(self, vocab_size):\n",
    "        # Call the parent class's constructor\n",
    "        super().__init__()\n",
    "        # Create an embedding table to map token indices to vectors\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Get the logits (unnormalized probabilities) for the input indices\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        # If targets are not provided, set the loss to None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        # If targets are provided, calculate the cross-entropy loss\n",
    "        else:\n",
    "            # Get the shape of the logits tensor (batch size, sequence length, vocab size)\n",
    "            B, T, C = logits.shape\n",
    "            # Reshape the logits tensor to (batch size * sequence length, vocab size)\n",
    "            logits = logits.view(B*T, C)\n",
    "            # Reshape the targets tensor to (batch size * sequence length)\n",
    "            targets = targets.view(B*T)\n",
    "            # Calculate the cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        # Return the logits and loss\n",
    "        return logits, loss\n",
    "\n",
    "    # Define the generate method\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Generate new tokens one at a time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the logits and loss for the current input indices\n",
    "            logits, loss = self.forward(idx)\n",
    "            # Get the logits for the last token in the sequence\n",
    "            logits = logits[:, -1, :]\n",
    "            # Convert the logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample the next token from the probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append the next token to the current sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # Return the generated sequence\n",
    "        return idx\n",
    "\n",
    "# Create an instance of the BigramLanguageModel with the specified vocabulary size\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "# Move the model to the specified device (e.g. GPU)\n",
    "m = model.to(device)\n",
    "\n",
    "# Create a context tensor with a single token (index 0)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# Generate a sequence of 100 tokens starting from the context\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=100)[0].tolist())\n",
    "# Print the generated sequence\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cad24d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.9653449058532715\n",
      "loss: 5.223823547363281\n",
      "loss: 5.172461032867432\n",
      "loss: 5.214517593383789\n",
      "loss: 5.067525386810303\n",
      "loss: 5.29563570022583\n",
      "loss: 5.170039653778076\n",
      "loss: 4.969202995300293\n",
      "loss: 4.522692680358887\n",
      "loss: 4.965234756469727\n",
      "loss: 4.814116954803467\n",
      "loss: 5.387789249420166\n",
      "loss: 4.864743709564209\n",
      "loss: 4.696987152099609\n",
      "loss: 4.775582313537598\n",
      "loss: 4.4776716232299805\n",
      "loss: 4.636866092681885\n",
      "loss: 4.858633518218994\n",
      "loss: 4.790683746337891\n",
      "loss: 4.800593852996826\n",
      "loss: 4.9609856605529785\n",
      "loss: 4.563626766204834\n",
      "loss: 4.451657772064209\n",
      "loss: 4.556346416473389\n",
      "loss: 4.485131740570068\n",
      "loss: 4.627432823181152\n",
      "loss: 4.526220321655273\n",
      "loss: 4.364133358001709\n",
      "loss: 4.580646514892578\n",
      "loss: 4.521752834320068\n",
      "loss: 4.125420570373535\n",
      "loss: 4.513637542724609\n",
      "loss: 4.395913124084473\n",
      "loss: 4.244991779327393\n",
      "loss: 4.228196144104004\n",
      "loss: 4.177089691162109\n",
      "loss: 4.288183689117432\n",
      "loss: 4.109536170959473\n",
      "loss: 4.178134918212891\n",
      "loss: 4.080709934234619\n",
      "loss: 4.023847579956055\n",
      "loss: 3.991612672805786\n",
      "loss: 4.306038856506348\n",
      "loss: 4.3196306228637695\n",
      "loss: 4.120784282684326\n",
      "loss: 4.287028789520264\n",
      "loss: 3.903705596923828\n",
      "loss: 4.113412857055664\n",
      "loss: 4.297202110290527\n",
      "loss: 3.778275489807129\n"
     ]
    }
   ],
   "source": [
    "max_iter = 5000\n",
    "learning_rate = 3e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    # Get a batch of training data\n",
    "    x, y = get_batch('train')\n",
    "    # Get the logits and loss\n",
    "    logits, loss = model(x, y)\n",
    "    # Backpropagate the loss\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print the loss every 100 iterations\n",
    "    if iter % 100 == 0:\n",
    "        print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8b85e",
   "metadata": {},
   "source": [
    "MSE (Mean Squared Error):\n",
    "MSE is a commonly used loss function in regression problems. It measures the average squared difference between the predicted and actual values. The lower the MSE, the better the model's performance.\n",
    "\n",
    "GD (Gradient Descent):\n",
    "GD is an optimization algorithm used to minimize the loss function of a model. It iteratively updates the model's parameters in the direction of steepest descent of the loss function. GD can be slow for large datasets as it requires computing the gradients for the entire dataset in each iteration.\n",
    "\n",
    "Momentum:\n",
    "Momentum is an extension of GD that helps accelerate convergence and overcome local minima. It introduces a momentum term that accumulates the gradients of previous iterations and uses it to update the model's parameters. This helps the model to continue moving in the right direction even when the gradients are small.\n",
    "\n",
    "RMSprop (Root Mean Square Propagation):\n",
    "RMSprop is an optimization algorithm that adapts the learning rate for each parameter based on the average of recent squared gradients. It helps to speed up convergence by reducing the learning rate for parameters with large gradients and increasing it for parameters with small gradients.\n",
    "\n",
    "Adam:\n",
    "Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of both momentum and RMSprop. It maintains a running average of both the gradients and the squared gradients, and uses them to update the model's parameters. Adam is known for its fast convergence and good performance on a wide range of problems.\n",
    "\n",
    "AdamW:\n",
    "AdamW is a variant of the Adam optimizer that incorporates weight decay regularization. Weight decay helps prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. AdamW is particularly effective when dealing with models with large numbers of parameters.\n",
    "\n",
    "These optimization algorithms are commonly used in machine learning and deep learning to train models and find the optimal set of parameters. The choice of optimizer depends on the specific problem and the characteristics of the dataset. Experimentation and tuning are often required to find the best optimizer for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b42836",
   "metadata": {},
   "source": [
    "#### Optimization Algorithms and Loss Functions\n",
    "\n",
    "**MSE (Mean Squared Error):** MSE is a commonly used loss function in regression problems. It measures the average squared difference between the predicted and actual values. The lower the MSE, the better the model's performance.\n",
    "\n",
    "**GD (Gradient Descent):** GD is an optimization algorithm used to minimize the loss function of a model. It iteratively updates the model's parameters in the direction of steepest descent of the loss function. GD can be slow for large datasets as it requires computing the gradients for the entire dataset in each iteration.\n",
    "\n",
    "**Momentum:** Momentum is an extension of GD that helps accelerate convergence and overcome local minima. It introduces a momentum term that accumulates the gradients of previous iterations and uses it to update the model's parameters. This helps the model to continue moving in the right direction even when the gradients are small.\n",
    "\n",
    "**RMSprop (Root Mean Square Propagation):** RMSprop is an optimization algorithm that adapts the learning rate for each parameter based on the average of recent squared gradients. It helps to speed up convergence by reducing the learning rate for parameters with large gradients and increasing it for parameters with small gradients.\n",
    "\n",
    "**Adam:** Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of both momentum and RMSprop. It maintains a running average of both the gradients and the squared gradients, and uses them to update the model's parameters. Adam is known for its fast convergence and good performance on a wide range of problems.\n",
    "\n",
    "**AdamW:** AdamW is a variant of the Adam optimizer that incorporates weight decay regularization. Weight decay helps prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. AdamW is particularly effective when dealing with models with large numbers of parameters.\n",
    "\n",
    "These optimization algorithms are commonly used in machine learning and deep learning to train models and find the optimal set of parameters. The choice of optimizer depends on the specific problem and the characteristics of the dataset. Experimentation and tuning are often required to find the best optimizer for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40251cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FoP‘é…pb1ocEOLboI™XffX8éJU™4Z9cmovY/,wV$%r2y(LRKR’rst0FioE…M’,py Ofzbd4r$m)fzOA‘mMb16g Um5’lkFHU8[v0\n"
     ]
    }
   ],
   "source": [
    "# Create a context tensor with a single token (index 0)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# Generate a sequence of 100 tokens starting from the context\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=100)[0].tolist())\n",
    "# Print the generated sequence\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc13008",
   "metadata": {},
   "source": [
    "#### Block Size and Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72bfb9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNOTE:\\n- The block size determines the size of each block in the tensor.\\n- The batch size determines the number of blocks to be processed in parallel.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "- The block size determines the size of each block in the tensor.\n",
    "- The batch size determines the number of blocks to be processed in parallel.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
