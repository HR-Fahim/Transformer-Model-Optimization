{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac952fd",
   "metadata": {},
   "source": [
    "#### Torch Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e26f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693cc813",
   "metadata": {},
   "source": [
    "#### CUDA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3c84dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd7bd7",
   "metadata": {},
   "source": [
    "#### Hyperperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1253e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "block_size = 64\n",
    "n_layer = 4 \n",
    "n_head = 8\n",
    "n_embd = 256\n",
    "\n",
    "print(n_embd // n_head) # Test purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27eed6",
   "metadata": {},
   "source": [
    "#### Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a8a7dcf-c82f-4f76-8625-01974fb32265",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"\"\n",
    "\n",
    "# Read the contents of the file 'infinite_in_modern_thought.txt' and store it in the variable 'text'\n",
    "with open('infinite_in_modern_thought.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print the first 1000 characters of the text\n",
    "# print(text[:1000])\n",
    "\n",
    "# Create a list of unique characters in the text and sort them\n",
    "chars = sorted(list(set(text)))\n",
    "# print(chars)\n",
    "\n",
    "# Print the number of unique characters\n",
    "# print(len(chars))\n",
    "\n",
    "# print(text[:1000])\n",
    "chars = sorted(list(set(text)))\n",
    "# print(chars)\n",
    "# print(len(chars))\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922e2a5",
   "metadata": {},
   "source": [
    "#### Simple Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "569fea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 61, 68, 68, 71, 11, 1, 50, 71, 74, 68, 60, 2]\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary that maps each character to its index in the 'chars' list\n",
    "string_to_index = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a dictionary that maps each index to its corresponding character in the 'chars' list\n",
    "index_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Define the 'endcode' function that converts a string to a list of character indices\n",
    "endcode = lambda s: [string_to_index[c] for c in s]\n",
    "\n",
    "# Define the 'decode' function that converts a list of character indices to a string\n",
    "decode = lambda l: ''.join([index_to_string[i] for i in l])\n",
    "\n",
    "print(endcode(\"Hello, World!\"))\n",
    "\n",
    "print(decode(endcode(\"Hello, World!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674aee3",
   "metadata": {},
   "source": [
    "#### Torch Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99153e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65, 76, 61,  1, 65, 70,  1, 69, 71, 60, 61, 74, 70,  1,\n",
      "        76, 64, 71, 77, 63, 64, 76,  0,  1,  1,  1,  1,  0, 47, 64, 65, 75,  1,\n",
      "        61, 58, 71, 71, 67,  1, 65, 75,  1, 62])\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary that maps each character to its index in the 'chars' list\n",
    "string_to_index = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a dictionary that maps each index to its corresponding character in the 'chars' list\n",
    "index_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Define the 'endcode' function that converts a string to a list of character indices\n",
    "endcode = lambda s: [string_to_index[c] for c in s]\n",
    "\n",
    "# Define the 'decode' function that converts a list of character indices to a string\n",
    "decode = lambda l: ''.join([index_to_string[i] for i in l])\n",
    "\n",
    "# Create a tensor from the encoded text using torch.tensor\n",
    "data = torch.tensor(endcode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ebb88",
   "metadata": {},
   "source": [
    "#### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33186e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets\n",
    "data_size = int(0.9 * len(data))  # Calculate the size of the train data\n",
    "\n",
    "train_data = data[:data_size]  # Assign the first 90% of the data to the train_data variable\n",
    "val_data = data[data_size:]  # Assign the remaining 10% of the data to the val_data variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aca5f",
   "metadata": {},
   "source": [
    "#### Tensor Process Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97864467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([97]), the target: 47\n",
      "when input is tensor([97, 47]), the target: 64\n",
      "when input is tensor([97, 47, 64]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1]), the target: 43\n",
      "when input is tensor([97, 47, 64, 61,  1, 43]), the target: 74\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74]), the target: 71\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71]), the target: 66\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61]), the target: 59\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59]), the target: 76\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1]), the target: 34\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34]), the target: 77\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77]), the target: 76\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61]), the target: 70\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70]), the target: 58\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61]), the target: 74\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74]), the target: 63\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61]), the target: 29\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29]), the target: 71\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71]), the target: 71\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71]), the target: 67\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1]), the target: 71\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71]), the target: 62\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1]), the target: 42\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42]), the target: 70\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1]), the target: 76\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76]), the target: 64\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1]), the target: 76\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76]), the target: 64\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61]), the target: 71\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71]), the target: 74\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74]), the target: 81\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1]), the target: 71\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71]), the target: 62\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1]), the target: 76\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76]), the target: 64\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1]), the target: 65\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65]), the target: 70\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70]), the target: 62\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62]), the target: 65\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65]), the target: 70\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70]), the target: 65\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65]), the target: 76\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65, 76]), the target: 61\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65, 76, 61]), the target: 1\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65, 76, 61,  1]), the target: 65\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65, 76, 61,  1, 65]), the target: 70\n",
      "when input is tensor([97, 47, 64, 61,  1, 43, 74, 71, 66, 61, 59, 76,  1, 34, 77, 76, 61, 70,\n",
      "        58, 61, 74, 63,  1, 61, 29, 71, 71, 67,  1, 71, 62,  1, 42, 70,  1, 76,\n",
      "        64, 61,  1, 76, 64, 61, 71, 74, 81,  1, 71, 62,  1, 76, 64, 61,  1, 65,\n",
      "        70, 62, 65, 70, 65, 76, 61,  1, 65, 70]), the target: 1\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    # Get the context by slicing the input sequence up to the current position\n",
    "    context = x[:t + 1]\n",
    "    \n",
    "    # Get the target by selecting the next character in the input sequence\n",
    "    target = y[t]\n",
    "    \n",
    "    # Print the context and target\n",
    "    print(f\"when input is {context}, the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248bc77",
   "metadata": {},
   "source": [
    "#### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc93cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64]) torch.Size([128, 64])\n",
      "tensor([[61, 70, 76,  ..., 76, 74, 71],\n",
      "        [61,  1, 57,  ..., 65, 75,  1],\n",
      "        [65, 70,  1,  ...,  1, 71, 74],\n",
      "        ...,\n",
      "        [61,  1, 75,  ..., 75,  1, 69],\n",
      "        [75,  1, 71,  ..., 71, 70, 78],\n",
      "        [60, 74, 61,  ..., 71, 70,  1]], device='cuda:0')\n",
      "tensor([[70, 76,  1,  ..., 74, 71, 70],\n",
      "        [ 1, 57, 75,  ..., 75,  1, 57],\n",
      "        [70,  1, 81,  ..., 71, 74,  1],\n",
      "        ...,\n",
      "        [ 1, 75, 61,  ...,  1, 69, 77],\n",
      "        [ 1, 71, 62,  ..., 70, 78, 61],\n",
      "        [74, 61, 75,  ..., 70,  1, 90]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and validation sets\n",
    "data_size = int(0.9 * len(data))  # Calculate the size of the train data\n",
    "\n",
    "train_data = data[:data_size]  # Assign the first 90% of the data to the train_data variable\n",
    "val_data = data[data_size:]  # Assign the remaining 10% of the data to the val_data variable\n",
    "\n",
    "# Define the get_batch function that returns a batch of data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Get a batch of training data\n",
    "x, y = get_batch('train')\n",
    "\n",
    "# Print the shape of x and y\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# Print the values of x and y\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee42a2",
   "metadata": {},
   "source": [
    "#### Bigram (Autoregressive Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc2e5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, time-step, channels)\n",
    "        # Output shape: (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, hs)\n",
    "        q = self.query(x) # (B, T, hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # (B, T, hs) @ (B, T, T) -> (B, hs, T)\n",
    "        v = self.value(x) # (B, T, hs)\n",
    "        out = wei @ v # (B, T, hs)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "727e9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention layer.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(head_size) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.proj = nn.Linear(head_size * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B,T, F) -> (B,T, (h1, h1, h2, h2, h3, h3, h4, h4))\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1c4b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72b25c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: A multi-head attention layer followed by a feedforward layer.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd is the embedding dimension, n_head is the number of attention heads\n",
    "        # n_head is the number of attention heads\n",
    "\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = nn.MultiheadAttention(embed_dim=n_embd, num_heads=n_head)\n",
    "\n",
    "        # self.ffn = nn.Sequential(\n",
    "        #     nn.Linear(n_embd, 4 * n_embd),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(4 * n_embd, n_embd),\n",
    "        # )\n",
    "\n",
    "        self.ffw = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffw(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc3ebc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    # Initialize the model with the vocabulary size\n",
    "    def __init__(self, vocab_size):\n",
    "        # Call the parent class's constructor\n",
    "        super().__init__()\n",
    "        # Create an embedding table to map token indices to vectors\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Create a multi-head attention layer (GPU/CPU)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head)\n",
    "              for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "        self.lm_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self.__init_weights)\n",
    "\n",
    "    def __init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Get the logits (unnormalized probabilities) for the input indices\n",
    "        # logits = self.token_embedding_table(idx)\n",
    "\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.lm_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        # If targets are not provided, set the loss to None\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        # If targets are provided, calculate the cross-entropy loss\n",
    "        else:\n",
    "            # Get the shape of the logits tensor (batch size, sequence length, vocab size)\n",
    "            B, T, C = logits.shape\n",
    "            # Reshape the logits tensor to (batch size * sequence length, vocab size)\n",
    "            logits = logits.view(B*T, C)\n",
    "            # Reshape the targets tensor to (batch size * sequence length)\n",
    "            targets = targets.view(B*T)\n",
    "            # Calculate the cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        # Return the logits and loss\n",
    "        return logits, loss\n",
    "\n",
    "    # Define the generate method\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Generate new tokens one at a time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the logits and loss for the current input indices\n",
    "            logits, loss = self.forward(idx)\n",
    "            # Get the logits for the last token in the sequence\n",
    "            logits = logits[:, -1, :]\n",
    "            # Convert the logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample the next token from the probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Append the next token to the current sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # Return the generated sequence\n",
    "        return idx\n",
    "    \n",
    "model = GPTLanguageModel(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cad24d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MultiheadAttention.forward() missing 2 required positional arguments: 'key' and 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Get a batch of training data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 17\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m get_batch(split)  \u001b[38;5;66;03m# Get a batch of data\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     T \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Define the variable 'T' as the sequence length\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     losses[i] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Store the loss value\u001b[39;00m\n\u001b[0;32m     19\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# Calculate the mean loss for the split\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 38\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     36\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T, C)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_f(x) \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[34], line 23\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 23\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x \u001b[38;5;241m+\u001b[39m y)\n\u001b[0;32m     25\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffw(x)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Desktop\\Research\\Project\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: MultiheadAttention.forward() missing 2 required positional arguments: 'key' and 'value'"
     ]
    }
   ],
   "source": [
    "max_iter = 500\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 100\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_interval)  # Initialize a tensor to store the losses\n",
    "        for i in range(eval_interval):\n",
    "            x, y = get_batch(split)  # Get a batch of data\n",
    "            T = x.shape[1]  # Define the variable 'T' as the sequence length\n",
    "            logits, loss = model(x, y)  # Forward pass through the model\n",
    "            losses[i] = loss.item()  # Store the loss value\n",
    "        out[split] = losses.mean()  # Calculate the mean loss for the split\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return out\n",
    "\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    # Get a batch of training data\n",
    "    x, y = get_batch('train')\n",
    "    # Get the logits and loss\n",
    "    logits, loss = model(x, y)\n",
    "    # Backpropagate the loss\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print the loss every 100 iterations\n",
    "    # if iter % 100 == 0:\n",
    "    #     print(f\"loss: {loss.item()}\")\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b42836",
   "metadata": {},
   "source": [
    "#### Some Optimization Algorithms and Loss Functions\n",
    "\n",
    "**MSE (Mean Squared Error):** MSE is a commonly used loss function in regression problems. It measures the average squared difference between the predicted and actual values. The lower the MSE, the better the model's performance.\n",
    "\n",
    "**GD (Gradient Descent):** GD is an optimization algorithm used to minimize the loss function of a model. It iteratively updates the model's parameters in the direction of steepest descent of the loss function. GD can be slow for large datasets as it requires computing the gradients for the entire dataset in each iteration.\n",
    "\n",
    "**Momentum:** Momentum is an extension of GD that helps accelerate convergence and overcome local minima. It introduces a momentum term that accumulates the gradients of previous iterations and uses it to update the model's parameters. This helps the model to continue moving in the right direction even when the gradients are small.\n",
    "\n",
    "**RMSprop (Root Mean Square Propagation):** RMSprop is an optimization algorithm that adapts the learning rate for each parameter based on the average of recent squared gradients. It helps to speed up convergence by reducing the learning rate for parameters with large gradients and increasing it for parameters with small gradients.\n",
    "\n",
    "**Adam:** Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of both momentum and RMSprop. It maintains a running average of both the gradients and the squared gradients, and uses them to update the model's parameters. Adam is known for its fast convergence and good performance on a wide range of problems.\n",
    "\n",
    "**AdamW:** AdamW is a variant of the Adam optimizer that incorporates weight decay regularization. Weight decay helps prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. AdamW is particularly effective when dealing with models with large numbers of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40251cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a context tensor with a single token (index 0)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# Generate a sequence of 100 tokens starting from the context\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=100)[0].tolist())\n",
    "# Print the generated sequence\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc13008",
   "metadata": {},
   "source": [
    "#### Block Size and Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "- The block size determines the size of each block in the tensor.\n",
    "- The batch size determines the number of blocks to be processed in parallel.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
